# Kaggle_Titanic_Competition-Top7%-
https://www.kaggle.com/patricksu/titanic-feature-engineering-ensemble-v3



## Practice for data cleaning, feature engineering, hyperparameter tuning and ensembling.

Predictions of the survival rate of Titanic disaster were made by different models:
- LGBM
- XGboost
- Random Forest
- SVM
- Logist Regression
Ensembling models can further enhance the performance, and I tried two ensemble method:
- Averaging
- Voting
The voting method got the best result which ranked Top 7% in Kaggle LeaderBoard.

However, there are still lots of ensemble method worth trying, for example, boosting, bagging, stacking...etc
link below:

https://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-for-ensemble-models/




